{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zemWtDr8gVOf"
      },
      "source": [
        "![Banner](https://i.imgur.com/a3uAqnb.png)\n",
        "\n",
        "# Building and Optimizing a CNN - Homework Assignment\n",
        "\n",
        "In this homework, you will design, implement, and optimize a **Convolutional Neural Network (CNN)** using PyTorch to classify images from the CIFAR-10 dataset. This will involve advanced preprocessing techniques, sophisticated model architectures, hyperparameter tuning, and comprehensive evaluation.\n",
        "\n",
        "## üìå Project Overview\n",
        "- **Task**: Image classification on CIFAR-10 dataset\n",
        "- **Architecture**: CNN with Inception bottlenecks and advanced optimizations\n",
        "- **Dataset**: CIFAR-10 (60,000 32x32 color images in 10 classes)\n",
        "- **Goal**: Achieve high classification accuracy with optimized training\n",
        "\n",
        "## üìö Learning Objectives\n",
        "By completing this assignment, you will:\n",
        "- Implement advanced data augmentation techniques\n",
        "- Design complex CNN architectures with Inception bottlenecks\n",
        "- Compare different optimizers and learning rate schedules\n",
        "- Apply various regularization techniques\n",
        "- Evaluate models with comprehensive metrics\n",
        "- Visualize training progress and results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Import Libraries and Configuration\n",
        "\n",
        "**Task**: Import all necessary libraries and set up configuration parameters.\n",
        "\n",
        "**Requirements**:\n",
        "- Import PyTorch, torchvision, and related libraries\n",
        "- Import matplotlib, numpy, and other utilities\n",
        "- Set random seeds for reproducibility\n",
        "- Configure hyperparameters with reasonable values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import all necessary libraries:\n",
        "#       - torch, torch.nn, torch.optim\n",
        "#       - torchvision, torchvision.transforms\n",
        "#       - matplotlib.pyplot, numpy\n",
        "#       - sklearn.metrics for advanced metrics\n",
        "#       - Other utilities as needed\n",
        "\n",
        "# TODO: Set random seeds for reproducibility (use seed=42)\n",
        "#       - torch.manual_seed(42)\n",
        "#       - np.random.seed(42)\n",
        "#       - torch.cuda.manual_seed(42) if using GPU\n",
        "\n",
        "# TODO: Check device availability and print\n",
        "\n",
        "# TODO: Define configuration parameters:\n",
        "BATCH_SIZE = 128  # Batch size for training\n",
        "LEARNING_RATE = 0.001  # Initial learning rate\n",
        "NUM_EPOCHS = 50  # Number of training epochs\n",
        "NUM_CLASSES = 10  # CIFAR-10 has 10 classes\n",
        "INPUT_SIZE = 32  # CIFAR-10 image size is 32x32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Load and Preprocess the Data\n",
        "\n",
        "**Task**: Load CIFAR-10 dataset and implement advanced preprocessing techniques.\n",
        "\n",
        "**Requirements**:\n",
        "- Load CIFAR-10 training and test sets\n",
        "- Apply data normalization using dataset statistics\n",
        "- Implement comprehensive data augmentation for training\n",
        "- Create data loaders with appropriate settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define data transforms for training (with augmentation):\n",
        "#       - RandomHorizontalFlip(p=0.5)\n",
        "#       - RandomRotation(degrees=10)\n",
        "#       - RandomCrop(32, padding=4)\n",
        "#       - ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
        "#       - ToTensor()\n",
        "#       - Normalize with CIFAR-10 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "# TODO: Define data transforms for testing (no augmentation):\n",
        "#       - ToTensor()\n",
        "#       - Normalize with same values as training\n",
        "\n",
        "# TODO: Load CIFAR-10 datasets:\n",
        "#       - trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "#       - testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "# TODO: Create data loaders:\n",
        "#       - trainloader with shuffle=True\n",
        "#       - testloader with shuffle=False\n",
        "\n",
        "# TODO: Define CIFAR-10 class names\n",
        "# TODO: Print dataset information (sizes, classes, etc.)\n",
        "# TODO: Visualize some sample images with their labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Design Complex CNN Architecture with Inception Bottlenecks\n",
        "\n",
        "**Task**: Implement a sophisticated CNN architecture incorporating Inception-style bottleneck blocks.\n",
        "\n",
        "**Requirements**:\n",
        "- Create an Inception bottleneck module with multiple parallel paths\n",
        "- Design the main CNN with multiple Inception blocks\n",
        "- Use appropriate pooling, batch normalization, and dropout\n",
        "- Implement skip connections where beneficial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create InceptionBottleneck class inheriting from nn.Module\n",
        "# TODO: In __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, out_pool):\n",
        "#       Build four parallel paths:\n",
        "#       Path 1: 1x1 convolution\n",
        "#       - Conv2d(in_channels, out_1x1, 1) + BatchNorm2d + ReLU\n",
        "#       \n",
        "#       Path 2: 1x1 reduction + 3x3 convolution  \n",
        "#       - Conv2d(in_channels, reduce_3x3, 1) + BatchNorm2d + ReLU\n",
        "#       - Conv2d(reduce_3x3, out_3x3, 3, padding=1) + BatchNorm2d + ReLU\n",
        "#       \n",
        "#       Path 3: 1x1 reduction + 5x5 convolution\n",
        "#       - Conv2d(in_channels, reduce_5x5, 1) + BatchNorm2d + ReLU  \n",
        "#       - Conv2d(reduce_5x5, out_5x5, 5, padding=2) + BatchNorm2d + ReLU\n",
        "#       \n",
        "#       Path 4: 3x3 max pooling + 1x1 projection\n",
        "#       - MaxPool2d(3, stride=1, padding=1)\n",
        "#       - Conv2d(in_channels, out_pool, 1) + BatchNorm2d + ReLU\n",
        "\n",
        "# TODO: In forward(self, x):\n",
        "#       - Pass input through all four paths\n",
        "#       - Concatenate outputs along channel dimension\n",
        "#       - Return concatenated result\n",
        "\n",
        "# TODO: Create main CNN class inheriting from nn.Module\n",
        "# TODO: In __init__(self, num_classes=10):\n",
        "#       Initial layers:\n",
        "#       - Conv2d(3, 64, 3, padding=1) + BatchNorm2d + ReLU\n",
        "#       - Conv2d(64, 64, 3, padding=1) + BatchNorm2d + ReLU\n",
        "#       - MaxPool2d(2, 2)\n",
        "#       - Dropout2d(0.1)\n",
        "#       \n",
        "#       Inception blocks:\n",
        "#       - InceptionBottleneck(64, 16, 32, 64, 16, 32, 32)  # Output: 144 channels\n",
        "#       - InceptionBottleneck(144, 32, 64, 128, 32, 64, 64) # Output: 288 channels\n",
        "#       - MaxPool2d(2, 2)\n",
        "#       - Dropout2d(0.2)\n",
        "#       \n",
        "#       - InceptionBottleneck(288, 64, 128, 256, 64, 128, 128) # Output: 576 channels\n",
        "#       - InceptionBottleneck(576, 128, 256, 512, 128, 256, 256) # Output: 1152 channels\n",
        "#       - AdaptiveAvgPool2d((1, 1))\n",
        "#       \n",
        "#       Classifier:\n",
        "#       - Dropout(0.5)\n",
        "#       - Linear(1152, num_classes)\n",
        "\n",
        "# TODO: In forward(self, x):\n",
        "#       - Pass through all layers sequentially\n",
        "#       - Flatten before classifier\n",
        "#       - Return logits\n",
        "\n",
        "# TODO: Initialize model and move to device\n",
        "# TODO: Print model architecture and parameter count\n",
        "# TODO: Test with random input to verify output shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Implement and Compare Different Optimizers\n",
        "\n",
        "**Task**: Set up multiple optimizers and compare their performance.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement SGD, Adam, and AdamW optimizers\n",
        "- Use appropriate hyperparameters for each\n",
        "- Create a function to easily switch between optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create function get_optimizer(model, optimizer_name, learning_rate):\n",
        "#       Support the following optimizers:\n",
        "#       - 'sgd': SGD with momentum=0.9, weight_decay=1e-4\n",
        "#       - 'adam': Adam with betas=(0.9, 0.999), weight_decay=1e-4\n",
        "#       - 'adamw': AdamW with betas=(0.9, 0.999), weight_decay=1e-2\n",
        "#       Return the selected optimizer\n",
        "\n",
        "# TODO: Initialize your chosen optimizer (recommend starting with 'adamw')\n",
        "# TODO: Print optimizer configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Use Learning Rate Scheduling\n",
        "\n",
        "**Task**: Implement learning rate scheduling for improved training dynamics.\n",
        "\n",
        "**Requirements**:\n",
        "- Use StepLR or CosineAnnealingLR scheduler\n",
        "- Configure appropriate scheduling parameters\n",
        "- Track learning rate changes during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create learning rate scheduler:\n",
        "#       Option 1: StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "#       Option 2: CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
        "#       Choose one and justify your choice\n",
        "\n",
        "# TODO: Create function to get current learning rate from optimizer\n",
        "# TODO: Initialize lists to track learning rates during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Apply Regularization Techniques  \n",
        "\n",
        "**Task**: Implement various regularization methods to prevent overfitting.\n",
        "\n",
        "**Requirements**:\n",
        "- Use dropout in your model (already included in architecture)\n",
        "- Implement early stopping mechanism\n",
        "- Add L2 weight decay (already in optimizer)\n",
        "- Optional: implement label smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create EarlyStopping class:\n",
        "#       - __init__(self, patience=7, min_delta=0, restore_best_weights=True)\n",
        "#       - __call__(self, val_loss, model) method that:\n",
        "#         * Checks if validation loss improved by min_delta\n",
        "#         * Increments counter if no improvement\n",
        "#         * Saves best model weights if improvement\n",
        "#         * Returns True if should stop (patience exceeded)\n",
        "\n",
        "# TODO: Initialize early stopping with patience=10\n",
        "\n",
        "# TODO: Optional: Create label smoothing loss function\n",
        "#       - LabelSmoothingCrossEntropy class with smoothing parameter\n",
        "#       - Mixes one-hot labels with uniform distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Training Loop with Advanced Features\n",
        "\n",
        "**Task**: Implement comprehensive training loop with all optimizations.\n",
        "\n",
        "**Requirements**:\n",
        "- Track multiple metrics during training\n",
        "- Implement proper validation\n",
        "- Save best model checkpoints\n",
        "- Monitor learning rate and loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define loss function (CrossEntropyLoss or LabelSmoothingCrossEntropy)\n",
        "\n",
        "# TODO: Initialize tracking lists for:\n",
        "#       - train_losses, val_losses\n",
        "#       - train_accuracies, val_accuracies  \n",
        "#       - learning_rates\n",
        "\n",
        "# TODO: Create training loop for NUM_EPOCHS:\n",
        "#       \n",
        "#       Training phase:\n",
        "#       - Set model to train mode\n",
        "#       - For each batch in trainloader:\n",
        "#         * Move data to device\n",
        "#         * Zero gradients\n",
        "#         * Forward pass\n",
        "#         * Calculate loss\n",
        "#         * Backward pass and optimize\n",
        "#         * Track running loss and accuracy\n",
        "#       \n",
        "#       Validation phase:\n",
        "#       - Set model to eval mode\n",
        "#       - With torch.no_grad():\n",
        "#         * Calculate validation loss and accuracy\n",
        "#         * Track metrics\n",
        "#       \n",
        "#       Scheduling and monitoring:\n",
        "#       - Step learning rate scheduler\n",
        "#       - Check early stopping\n",
        "#       - Print epoch statistics\n",
        "#       - Save best model if validation improved\n",
        "\n",
        "# TODO: Plot training curves (loss and accuracy)\n",
        "# TODO: Plot learning rate schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Evaluate Model with Advanced Metrics\n",
        "\n",
        "**Task**: Comprehensive evaluation using multiple metrics and visualizations.\n",
        "\n",
        "**Requirements**:\n",
        "- Calculate accuracy, precision, recall, F1-score\n",
        "- Generate confusion matrix\n",
        "- Analyze per-class performance\n",
        "- Visualize misclassified examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load best model weights\n",
        "\n",
        "# TODO: Create evaluation function that calculates:\n",
        "#       - Overall accuracy\n",
        "#       - Per-class accuracy\n",
        "#       - Precision, recall, F1-score (macro and weighted averages)\n",
        "#       - Confusion matrix\n",
        "\n",
        "# TODO: Generate predictions on test set:\n",
        "#       - Set model to eval mode\n",
        "#       - Collect all predictions and true labels\n",
        "#       - Calculate all metrics using sklearn.metrics\n",
        "\n",
        "# TODO: Create confusion matrix visualization:\n",
        "#       - Use seaborn heatmap or matplotlib imshow\n",
        "#       - Add class names as labels\n",
        "#       - Display percentages and counts\n",
        "\n",
        "# TODO: Display classification report with per-class metrics\n",
        "\n",
        "# TODO: Find and visualize misclassified examples:\n",
        "#       - Identify worst performing classes\n",
        "#       - Show examples of incorrect predictions\n",
        "#       - Display true label vs predicted label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ Visualize Results\n",
        "\n",
        "**Task**: Create comprehensive visualizations of model performance and behavior.\n",
        "\n",
        "**Requirements**:\n",
        "- Plot training/validation curves\n",
        "- Visualize model predictions\n",
        "- Show sample activations or feature maps\n",
        "- Create performance comparison charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create comprehensive plotting function that shows:\n",
        "#       1. Training and validation loss curves\n",
        "#       2. Training and validation accuracy curves  \n",
        "#       3. Learning rate schedule\n",
        "#       4. Confusion matrix heatmap\n",
        "\n",
        "# TODO: Create prediction visualization function:\n",
        "#       - Show grid of test images with predicted vs true labels\n",
        "#       - Highlight correct (green) and incorrect (red) predictions\n",
        "#       - Display confidence scores\n",
        "\n",
        "# TODO: Optional: Visualize feature maps from convolutional layers:\n",
        "#       - Hook into intermediate layers\n",
        "#       - Show activation patterns for sample images\n",
        "#       - Compare activations across different classes\n",
        "\n",
        "# TODO: Optional: Create architecture diagram or summary visualization\n",
        "\n",
        "# TODO: Display all visualizations with proper titles and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Evaluation Criteria\n",
        "\n",
        "Your homework will be evaluated based on:\n",
        "\n",
        "1. **Implementation Correctness (40%)**\n",
        "   - Proper CNN architecture with Inception bottlenecks\n",
        "   - Correct data augmentation and preprocessing\n",
        "   - Working training loop with all optimizations\n",
        "\n",
        "2. **Training and Results (25%)**\n",
        "   - Model trains successfully without errors\n",
        "   - Achieves reasonable accuracy on CIFAR-10 (>80%)\n",
        "   - Proper use of regularization and scheduling\n",
        "\n",
        "3. **Code Quality (20%)**\n",
        "   - Clean, readable code with comprehensive comments\n",
        "   - Proper tensor handling and memory management\n",
        "   - Efficient implementation\n",
        "\n",
        "4. **Analysis and Visualization (15%)**\n",
        "   - Comprehensive evaluation with multiple metrics\n",
        "   - Clear visualizations of results and training progress\n",
        "\n",
        "**Bonus Points**:\n",
        "- Creative architectural improvements\n",
        "- Additional regularization techniques\n",
        "- Hyperparameter optimization\n",
        "- Ensemble methods or model averaging"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
