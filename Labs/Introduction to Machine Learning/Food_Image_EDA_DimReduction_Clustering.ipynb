{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefb4bf48bcdbea7",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3641d63174596",
   "metadata": {},
   "source": [
    "# Day 1: Exploring Food Images - EDA, Dimensionality Reduction & Clustering\n",
    "\n",
    "Welcome! In this notebook, we'll embark on an exploratory journey with a food image dataset. Our goal is not to build a supervised classifier (like in the CIFAR-10 example), but rather to understand the dataset's structure using unsupervised learning techniques.\n",
    "\n",
    "We will cover:\n",
    "1.  **Loading and Initial Exploration (EDA):** Getting familiar with the dataset, its classes, and visualizing some samples.\n",
    "2.  **Data Preprocessing:** Preparing the images (resizing, flattening, scaling) for machine learning algorithms.\n",
    "3.  **Dimensionality Reduction:** Reducing the high-dimensional image data to a lower-dimensional space to make it easier to work with and visualize. We'll look at PCA, UMAP, t-SNE, and Kernel PCA.\n",
    "4.  **Clustering:** Grouping similar images together using K-Means clustering on the reduced features and visualizing the results.\n",
    "\n",
    "Let's start by importing necessary libraries and loading the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for installing/upgrading required libraries if running in a new environment\n",
    "# !pip install --upgrade datasets fsspec huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb4b194c4c5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73601299fb43ee11",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Initial Setup\n",
    "\n",
    "We'll use the `datasets` library to load the \"Kaludi/data-food-classification\" dataset from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Kaludi/data-food-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32af48c8f6071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ed7775b2bc974",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before diving into complex processing, let's explore the dataset. We'll check:\n",
    "- The distribution of labels (classes) in the training and validation sets.\n",
    "- The unique label names and the total number of classes.\n",
    "- Sample images from each class.\n",
    "- Basic properties of the images (like size and mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7476e9c9877f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique labels from both train and validation\n",
    "train_labels_list = [item['label'] for item in ds['train']]\n",
    "val_labels_list = [item['label'] for item in ds['validation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03d741a7789dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train label distribution:\")\n",
    "print(Counter(train_labels_list))\n",
    "\n",
    "print(\"Validation label distribution:\")\n",
    "print(Counter(val_labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315d39b28c1ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique label names\n",
    "unique_labels = sorted(list(set(train_labels_list + val_labels_list)))\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da88b504cab4644",
   "metadata": {},
   "source": [
    "### 2.1 Visualizing Sample Images\n",
    "\n",
    "Let's define a function to display a few sample images for each food category to get a visual sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2763e61b86a9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples_by_label(dataset, dataset_name, samples_per_label=3):\n",
    "    \"\"\"Plot sample images for each label\"\"\"\n",
    "\n",
    "    # Group images by label\n",
    "    label_to_images = {}\n",
    "    for i, item in enumerate(dataset):\n",
    "        label = item['label']\n",
    "        if label not in label_to_images:\n",
    "            label_to_images[label] = []\n",
    "        label_to_images[label].append((i, item['image']))\n",
    "\n",
    "    # Create subplots\n",
    "    n_labels = len(label_to_images)\n",
    "    fig, axes = plt.subplots(n_labels, samples_per_label,\n",
    "                             figsize=(samples_per_label * 3, n_labels * 3))\n",
    "\n",
    "    if n_labels == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    fig.suptitle(f'{dataset_name} Dataset - Sample Images by Label', fontsize=16)\n",
    "\n",
    "    for label_idx, (label, images) in enumerate(sorted(label_to_images.items())):\n",
    "        # Take first few samples for this label\n",
    "        sample_images = images[:samples_per_label]\n",
    "\n",
    "        for img_idx, (orig_idx, image) in enumerate(sample_images):\n",
    "            ax = axes[label_idx, img_idx]\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f'Label: {label}\\nIndex: {orig_idx}')\n",
    "            ax.axis('off')\n",
    "\n",
    "        # Fill empty subplots if not enough samples\n",
    "        for img_idx in range(len(sample_images), samples_per_label):\n",
    "            axes[label_idx, img_idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989656316a2242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from training set\n",
    "plot_samples_by_label(ds['train'], 'Training', samples_per_label=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18944b54bc97dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from validation set\n",
    "plot_samples_by_label(ds['validation'], 'Validation', samples_per_label=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9442b8cd01e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image properties from first few samples\n",
    "print(f\"Sample image properties:\")\n",
    "for i in range(min(3, len(ds['train']))):\n",
    "    img = ds['train'][i]['image']\n",
    "    print(f\"Image {i}: Size = {img.size}, Mode = {img.mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936add72ad4466b",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "To use these images with dimensionality reduction and clustering algorithms, we need to preprocess them:\n",
    "1.  **Resize:** Ensure all images have a consistent size.\n",
    "2.  **Convert to Array & Flatten:** Convert PIL images to NumPy arrays and then flatten them into 1D vectors.\n",
    "3.  **Normalize:** Scale pixel values (typically to the 0-1 range).\n",
    "4.  **Combine & Scale:** Combine training and validation images and then apply feature scaling (e.g., `StandardScaler`).\n",
    "    - Why do you think we combine them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f92a3cb0f35f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128, 128)  # Resize all images to this size\n",
    "N_COMPONENTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099ce1f734ca437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_and_labels(dataset, dataset_name):\n",
    "    \"\"\"\n",
    "    Preprocess images: resize, convert to arrays, flatten\n",
    "    Keep track of labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    print(f\"Processing {dataset_name} set...\")\n",
    "\n",
    "    for i, item in enumerate(dataset):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Processed {i}/{len(dataset)} images\")\n",
    "\n",
    "        # Get image and label\n",
    "        img = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        # Resize image\n",
    "        img_resized = img.resize(TARGET_SIZE)\n",
    "\n",
    "        # Convert to numpy array and normalize to 0-1\n",
    "        img_array = np.array(img_resized) / 255.0\n",
    "\n",
    "        # Flatten the image (height * width * channels)\n",
    "        img_flattened = img_array.flatten()\n",
    "\n",
    "        images.append(img_flattened)\n",
    "        labels.append(label)\n",
    "\n",
    "    print(f\"  Completed processing {len(images)} images\")\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f061003b57ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = preprocess_images_and_labels(ds['train'], 'Training')\n",
    "val_images, val_labels = preprocess_images_and_labels(ds['validation'], 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4eaf7de86dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training images shape: {train_images.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Validation images shape: {val_images.shape}\")\n",
    "print(f\"Validation labels shape: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97018034ab4054",
   "metadata": {},
   "source": [
    "### 3.1 Combining and Scaling Data\n",
    "\n",
    "We'll combine the preprocessed training and validation images for dimensionality reduction and clustering. Then, we'll scale the features using `StandardScaler` to ensure each feature contributes equally to the distance computations in PCA and K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a74aa5945701f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = np.vstack([train_images, val_images])\n",
    "all_labels_combined = np.concatenate([train_labels, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d733bccc7f3fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "all_images_scaled = scaler.fit_transform(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34405a48fc2b41",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction\n",
    "\n",
    "Image data is high-dimensional (e.g., 128x128x3 = 49,152 features per image). Dimensionality reduction techniques help to:\n",
    "- Reduce computational complexity.\n",
    "- Mitigate the \"curse of dimensionality.\"\n",
    "- Potentially improve the performance of subsequent algorithms by removing noise.\n",
    "- Enable visualization in 2D or 3D. (If needed)\n",
    "We'll explore several techniques. Note that in this notebook, each subsequent technique is applied to the *scaled original data*, and the variable `all_images_reduced` will be updated. The final clustering will use the output of the last applied dimensionality reduction method. Of course you can mitigate this if you run only the reduction technique you want\n",
    "\n",
    "\n",
    "### 4.1 Principal Component Analysis (PCA)\n",
    "PCA is a linear technique that projects data onto a lower-dimensional subspace while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de74b221a0f9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "print(f\"Applying PCA with {N_COMPONENTS} components...\")\n",
    "pca = PCA(n_components=N_COMPONENTS)\n",
    "all_images_reduced = pca.fit_transform(all_images_scaled)\n",
    "print(f\"Original feature dimension: {all_images.shape[1]}\")\n",
    "print(f\"Explained variance ratio (first 10 components): {pca.explained_variance_ratio_[:10]}\")\n",
    "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"Shape after PCA: {all_images_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e68836e47afa7",
   "metadata": {},
   "source": [
    "### 4.2 Uniform Manifold Approximation and Projection (UMAP)\n",
    "UMAP is a non-linear dimensionality reduction technique effective for visualizing clusters and preserving both local and global structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638d5c2f4673593",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Applying UMAP with {N_COMPONENTS} components...\")\n",
    "reducer_umap = umap.UMAP(n_components=N_COMPONENTS,\n",
    "                         n_neighbors=15,  # Default is 15, controls local vs global structure\n",
    "                         min_dist=0.1,  # Default is 0.1, controls how tightly points are packed\n",
    "                         random_state=42)\n",
    "all_images_reduced = reducer_umap.fit_transform(all_images_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e4d51a5945c6c",
   "metadata": {},
   "source": [
    "### 4.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "t-SNE is another non-linear technique particularly good at visualizing high-dimensional datasets in low dimensions (typically 2D or 3D) by revealing local structure and clusters.\n",
    "*Note: t-SNE can be computationally intensive for large datasets and many components.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afa21b7bbeb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Applying t-SNE with {N_COMPONENTS} components...\")\n",
    "if N_COMPONENTS >= 4:\n",
    "    print(\"T-SNE requires 3 or less components, switching to 3 components...\")\n",
    "    tsne = TSNE(n_components=3,\n",
    "                perplexity=30,  # Common default, try values between 5 and 50\n",
    "                n_iter=1000,  # At least 250, more can be better\n",
    "                random_state=42,\n",
    "                n_jobs=-1)  # Use all available cores\n",
    "else:\n",
    "    tsne = TSNE(n_components=N_COMPONENTS,\n",
    "                perplexity=30,\n",
    "                max_iter=1000,  # At least 250, more can be better\n",
    "                random_state=42,\n",
    "                n_jobs=-1)  # Use all available cores\n",
    "all_images_reduced = tsne.fit_transform(all_images_scaled)\n",
    "print(f\"Shape after t-SNE: {all_images_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a6d1cd20223a0",
   "metadata": {},
   "source": [
    "### 4.4 Kernel PCA (KPCA)\n",
    "Kernel PCA extends PCA to non-linear dimensionality reduction by using kernel functions to project data into a higher-dimensional space where linear separation might be possible, before applying PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661874b65ed2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Applying Kernel PCA with {N_COMPONENTS} components...\")\n",
    "# Common kernels: 'linear', 'poly', 'rbf', 'sigmoid', 'cosine'\n",
    "# 'gamma' is a key parameter for 'rbf', 'poly', 'sigmoid'\n",
    "kpca = KernelPCA(n_components=N_COMPONENTS,\n",
    "                 kernel=\"rbf\",  # Radial Basis Function kernel is common\n",
    "                 gamma=None,  # If None, defaults to 1/n_features\n",
    "                 random_state=42,\n",
    "                 fit_inverse_transform=False)\n",
    "all_images_reduced = kpca.fit_transform(all_images_scaled)\n",
    "print(f\"Shape after Kernel PCA: {all_images_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2356bc0ada22b",
   "metadata": {},
   "source": [
    "## 5. Clustering with K-Means and Visualization\n",
    "\n",
    "After reducing the dimensionality of our image features, we can apply clustering algorithms to group similar images. We'll use K-Means, a popular partitioning algorithm.\n",
    "\n",
    "We'll define a function to:\n",
    "1.  Perform K-Means clustering.\n",
    "2.  Visualize random sample images from each discovered cluster.\n",
    "3.  Display the true label distribution within each cluster to see how well the unsupervised clustering aligns with the actual food categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7ff5699f2ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_and_visualize(features, true_labels, n_clusters, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering and visualize results with random samples from each cluster.\n",
    "    \"\"\"\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features)\n",
    "\n",
    "    print(f\"Cluster distribution: {Counter(cluster_labels)}\")\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(n_clusters, 6, figsize=(18, n_clusters * 3))\n",
    "    if n_clusters == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    fig.suptitle(f'Clustering Results - {n_clusters} Clusters {title_suffix}', fontsize=16)\n",
    "\n",
    "    # For each cluster, show sample images\n",
    "    for cluster_id in range(n_clusters):\n",
    "        # Get all original indices of data points belonging to this cluster\n",
    "        cluster_indices_all = np.where(cluster_labels == cluster_id)[0]\n",
    "\n",
    "        # Get true labels for this cluster (for statistics)\n",
    "        cluster_true_labels = [true_labels[i] for i in cluster_indices_all]\n",
    "\n",
    "        if not cluster_true_labels:  # Handle empty cluster if it occurs\n",
    "            for i in range(6):\n",
    "                axes[cluster_id, i].axis('off')\n",
    "            axes[cluster_id, 0].text(-0.5, 0.5, f'Cluster {cluster_id}\\nEmpty',\n",
    "                                     transform=axes[cluster_id, 0].transAxes,\n",
    "                                     bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"),\n",
    "                                     verticalalignment='center', fontsize=8)\n",
    "            continue\n",
    "\n",
    "        most_common_label = Counter(cluster_true_labels).most_common(1)[0][0]\n",
    "        label_counts = Counter(cluster_true_labels)\n",
    "\n",
    "        num_samples_to_show = 6\n",
    "        if len(cluster_indices_all) > num_samples_to_show:\n",
    "            sample_indices = random.sample(list(cluster_indices_all), num_samples_to_show)\n",
    "        else:\n",
    "            sample_indices = list(cluster_indices_all)\n",
    "\n",
    "\n",
    "        for i, sample_idx in enumerate(sample_indices):\n",
    "            ax = axes[cluster_id, i]\n",
    "\n",
    "            sample_idx_int = int(sample_idx)\n",
    "            if sample_idx_int < len(ds['train']):\n",
    "                original_img = ds['train'][sample_idx_int]['image']\n",
    "                dataset_type = \"Train\"\n",
    "            else:\n",
    "                val_idx = sample_idx_int - len(ds['train'])\n",
    "                original_img = ds['validation'][val_idx]['image']\n",
    "                dataset_type = \"Val\"\n",
    "\n",
    "            ax.imshow(original_img)\n",
    "            ax.set_title(f'True: {true_labels[sample_idx_int]}\\n{dataset_type}', fontsize=8)\n",
    "            ax.axis('off')\n",
    "\n",
    "        # Fill remaining subplots if fewer than 6 samples were available in the cluster\n",
    "        for i in range(len(sample_indices), num_samples_to_show):\n",
    "            axes[cluster_id, i].axis('off')\n",
    "\n",
    "        # Add cluster info as text on the left\n",
    "        cluster_info = f'Cluster {cluster_id}\\nSize: {len(cluster_indices_all)}\\nMain: {most_common_label}\\n'\n",
    "        cluster_info += '\\n'.join([f'{label}: {count}' for label, count in label_counts.most_common(3)])\n",
    "\n",
    "        # Add text box with cluster information\n",
    "        axes[cluster_id, 0].text(-0.5, 0.5, cluster_info,\n",
    "                                 transform=axes[cluster_id, 0].transAxes,\n",
    "                                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"),\n",
    "                                 verticalalignment='center', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return cluster_labels, kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b5ac414e0cd1b",
   "metadata": {},
   "source": [
    "### 5.1 Performing Clustering\n",
    "\n",
    "Let's run the K-Means algorithm. We'll try to find 7 clusters, which is the number of unique food categories we identified earlier. (Of course in real life you do not know the number of clusters, there are ways to find it, more about this [here](https://www.youtube.com/watch?v=ARjx__t5OCI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408aa113a502bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels_7, kmeans_7 = perform_clustering_and_visualize(\n",
    "    all_images_reduced, all_labels_combined, 7, \"(7 Clusters)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a57527d5e3552",
   "metadata": {},
   "source": [
    "## 6. Try these:\n",
    "- Experiment with different numbers of clusters for K-Means.\n",
    "- Evaluate clustering quality using metrics like Silhouette Score or Adjusted Rand Index (if ground truth is used for evaluation).\n",
    "- Try other clustering algorithms (e.g., DBSCAN, Agglomerative Clustering).\n",
    "- Use the reduced features from different DR techniques as input to the clustering step to compare their impact.\n",
    "- Build a supervised image classification model using these features or directly from the images using CNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
